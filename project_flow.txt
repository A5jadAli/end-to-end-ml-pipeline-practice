Building Pipeline:
> Create a Github repo and clone it in your local (add a folder named experiments that will contain initial data and notebook)
> Add src folder along with all components (run them individually)
> Add data, model, reports directories to .gitignore file
> Now git add, commit, push

Setting up dvc pipeline (without params):
> Create dvc.yaml file and add stages to it
> "dvc init" then do "dvc repro" to test the pipeline automation (do "dvc dag" to see how your pipeline components are connected)
> Now git add, commit, push

Setting up dvc pipeline (with params)
> add params.yaml file
> Add the params setup (mentioned below)
> add params in dvc.yaml if not added
> do "dvc repro" again to test the pipeline with params
> Now git add, commit, push

Experiments with dvc:
> pip install dvclive
> add the dvclive code block (mentioned below)
> do "dvc exp run", it will create a new dvc.yaml file (if already not there) and dvclive directory (each run will be considered as an experiment by DVC)
> do "dvc exp show" on terminal to see the experiments or use extension on VScode to see it
> do "dvc exp remove {exp-name}" to remove exp (optional) | "dvc exp apply {exp-name}" to reproduce prev exp
> change params, re-run code (produce new experiments)
> do git add, commit, push

Adding a remote S3 storage for DVC:
> login to aws console
> create an IAM user
> create S3 (use unique name for it)
> pip install dvc[s3]
> pip install awscli
> "aws configure"
> git remote add -d dvcstore s3://bucketname
> dvc commit, push the exp outcome that you want to use
> finally git add, commit, push


Extras:
> delete aws resources
> adding stages to dvc.yaml from terminal (if you don't wanna add stages maually in dvc.yaml file): "dvc stage add -n data_ingestion -d src/data_ingestion.py -o data/raw python src/data_ingestion.py"











------------------------------------------------------------------------------------------------------------
params.yaml setup:
1> import yaml
2> add func (right after logging):
def load_params(params_path: str) -> dict:
    """Load parameters from a YAML file."""
    try:
        with open(params_path, 'r') as file:
            params = yaml.safe_load(file)
        logger.debug('Parameters retrieved from %s', params_path)
        return params
    except FileNotFoundError:
        logger.error('File not found: %s', params_path)
        raise
    except yaml.YAMLError as e:
        logger.error('YAML error: %s', e)
        raise
    except Exception as e:
        logger.error('Unexpected error: %s', e)
        raise
3> Add to main():

# data_ingestion
params = load_params(params_path='params.yaml')
test_size = params['data_ingestion']['test_size']

# feature_engineering
params = load_params(params_path='params.yaml')
max_features = params['feature_engineering']['max_features']

# model_building
params = load_params('params.yaml')['model_building']

------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------

dvclive code block: (do this in model_evaluation)
1> import live and yaml:
from dvclive import Live
import yaml
2> Add the load_params function and initiate "params" var in main
3> Add below code block to main:
with Live(save_dvc_exp=True) as live:
    live.log_metric('accuracy', accuracy_score(y_test, y_test))
    live.log_metric('precision', precision_score(y_test, y_test))
    live.log_metric('recall', recall_score(y_test, y_test))

    live.log_params(params)
-----------------------------------------------------------------------------------------------------------